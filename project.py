# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

"""

import pandas as pd
import numpy as np
import seaborn as sns
sns.set()
from math import sqrt
from sklearn import svm, linear_model, ensemble, tree, model_selection, metrics
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import confusion_matrix, recall_score, precision_score, mean_squared_error, roc_auc_score, log_loss, accuracy_score, roc_curve
from sklearn.preprocessing import binarize, PolynomialFeatures, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression, RidgeClassifier, LinearRegression
from sklearn.calibration import calibration_curve

from mlxtend.feature_selection import SequentialFeatureSelector
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

url_test = 'https://drive.google.com/uc?export=download&id=1GviSK6fxuXvOZBLbELz2Y3yj_N9kSW4T'
data_test = pd.read_csv(url_test)
url_train = 'https://drive.google.com/uc?export=download&id=1tH8RA38R5HGdxOc4qFu07iKmOoUcZJ6Q'
data_train = pd.read_csv(url_train)

"""<h1>
At First Glance
</h1>
"""

data_train.dtypes

data_train.isnull().sum()

data_test.isnull().sum()

idCount = data_train.groupby(['id','date'],as_index=False).size().sort_values(ascending = False)
idCount = idCount.sort_values(ascending = False)
print(idCount)

data_train.describe()

data_test.describe()

def convertDate(a):
  MonthDay = [31,28,31,30,31,30,31,31,30,31,30,31]
  year = (int)(a[3]) - 4
  month = (int)(a[4:6])
  day = (int)(a[6:8])
  res = year*365
  for i in range(0,month-1):
    res+=MonthDay[i]
  res+=day
  return res  

data_train['dateInt'] = data_train['date'].apply(lambda x : convertDate(x))

sns.scatterplot(x='dateInt',y='price', data=data_train)

sns.scatterplot(x='bedrooms',y='price', data=data_train)

sns.scatterplot(x='bathrooms',y='price', data=data_train)

sns.scatterplot(x='sqft_living',y='price', data=data_train)

sns.scatterplot(x='sqft_lot',y='price', data=data_train)

sns.scatterplot(x='floors',y='price', data=data_train)

sns.scatterplot(x='waterfront',y='price', data=data_train)

sns.scatterplot(x='view',y='price', data=data_train)

sns.scatterplot(x='condition',y='price', data=data_train)

sns.scatterplot(x='grade',y='price', data=data_train)

sns.scatterplot(x='sqft_above',y='price', data=data_train)

sns.scatterplot(x='sqft_basement',y='price', data=data_train)

sns.scatterplot(x='yr_built',y='price', data=data_train)

data_train['yr_renovated'] = data_train.apply(lambda x: x['yr_renovated'] if x['yr_renovated'] > 0 else x['yr_built'], axis = 1)
sns.scatterplot(x='yr_renovated',y='price', data=data_train)

sns.scatterplot(x='zipcode',y='price', data=data_train)

sns.scatterplot(x='lat',y='price', data=data_train)

sns.scatterplot(x='long',y='price', data=data_train)

sns.scatterplot(x='sqft_living15',y='price', data=data_train)

sns.scatterplot(x='sqft_lot15',y='price', data=data_train)

"""<h1>
Look deeper into the data
</h1>
"""

geoCount = data_train.groupby(['lat','long'],as_index=False).size().sort_values(ascending = False)
print(geoCount)

postCodeCount = data_train.groupby(['zipcode'],as_index=False).size().sort_values(ascending = False)
print(postCodeCount)

"""<h1>
Preprocessing data
</h1>
"""

def preprocessingX(allData):
  allData['dateInt'] = allData['date'].apply(lambda x : convertDate(x))
  allData['yr_renovated'] = allData.apply(lambda x: x['yr_renovated'] if x['yr_renovated'] > 0 else x['yr_built'], axis = 1)
  allData['logSqft_Living']=np.log(allData['sqft_living'])
  allData['logSqft_Lot']=np.log(allData['sqft_lot'])
  allData['logSqft_Above']=np.log(allData['sqft_above'])
  allData['logSqft_Living15']=np.log(allData['sqft_living15'])
  allData['logSqft_Lot15']=np.log(allData['sqft_lot15'])
  allData['logSqft_Basement']=np.log(allData['sqft_basement'].apply(lambda x : np.e if x == 0 else x))
  qtyFeatures= ['dateInt','bedrooms','bathrooms','logSqft_Living','logSqft_Lot','floors','waterfront',	'logSqft_Above',
                'logSqft_Basement','yr_built',	'yr_renovated','logSqft_Living15','logSqft_Lot15']
  catFeatures = ['view', 'condition', 'grade','zipcode']
  X_encoded = encoder(allData, catFeatures, qtyFeatures)
  return X_encoded
  
def preprocessingY(allData):
  allData['unitPrice']=allData['price']/allData['sqft_living']
  allData['logPrice']=np.log(allData['price'])
  y_encoded = allData['logPrice']
  return y_encoded

def preprocessingData( allData):
  X_encoded = preprocessingX(allData)
  y_encoded = preprocessingY(allData)
  X_train, X_test, y_train, y_test = train_test_split(X_encoded,y_encoded,test_size = 0.2,random_state = 1)
  return X_encoded,y_encoded, X_train, X_test, y_train, y_test

def encoder(dataset, catFeatures, qtyFeatures):
  dataset = dataset[catFeatures + qtyFeatures]
  dataset_encoded = pd.get_dummies(dataset,columns = catFeatures,drop_first = True)
  return(dataset_encoded)

data_train.drop(data_train[data_train['bedrooms']>30].index, inplace=True)
X_encoded,y_encoded, X_train, X_test, y_train, y_test = preprocessingData( data_train)

"""<h1>
Use linear model to analysis
</h1>
"""

def linearRegressionPredict(X_train, y_train, X_test, y_test ):
  linreg = LinearRegression()
  linreg.fit(X_train, y_train)
  y_predicted_log = linreg.predict(X_test)
  y_predicted = np.e**(y_predicted_log)
  y_test_origin = np.e**(y_test)
  return y_predicted, y_predicted_log, y_test_origin

def get_rmse(test, predicted):
  rmse = sqrt(mean_squared_error(test, predicted))
  print(rmse)

y_predicted, y_predicted_log, y_test_origin = linearRegressionPredict(X_train, y_train, X_test, y_test )

log_rmse = get_rmse(y_test, y_predicted_log)
log_rmse

linreg = LinearRegression()
scores = cross_val_score(linreg, X_encoded, y_encoded, cv=10, scoring='neg_mean_squared_error')
cross_rmse = np.sqrt(-1 * scores)
print(np.mean(cross_rmse));

"""<h1>
Model comparation
</h1>
"""

def model_compare(X, y):
  Models = [
      # Linear Regression
      linear_model.LinearRegression(),
      linear_model.Lasso(),
      linear_model.Ridge(),
      # Tree
      tree.DecisionTreeRegressor(),
      # GBM
      ensemble.GradientBoostingRegressor()
  ]
  Model_Compare_Result = pd.DataFrame(columns=['Model_Name', 'Model_RMSE', 'Test_score'])

  i = 0
  for model in Models:
      y_pred = model_selection.cross_val_predict(model, X, y, cv=10)
      cv_res = model_selection.cross_validate(model, X, y, cv=10)
      scores = cross_val_score(model, X, y, cv=10, scoring='neg_mean_squared_error')
      cross_rmse = np.mean(np.sqrt(-1 * scores))
      Model_Compare_Result.loc[i, 'Model_Name'] = model.__class__.__name__
      Model_Compare_Result.loc[i, 'Model_RMSE'] = cross_rmse
      Model_Compare_Result.loc[i, 'Test_score'] = cv_res['test_score'].mean()
      i += 1

  Model_Compare_Result.sort_values(by=['Model_RMSE'], ascending=True, inplace=True)
  return Model_Compare_Result

Model_Compare_Result =model_compare(X_encoded,y_encoded)
Model_Compare_Result

"""<h1>
The polynomial feature and feature selection peform really slow when we have 101 features, so we decide to reduce zipcode into several groups, to reduce the feature number significantly
</h1>
"""

data_train['unitPrice']=data_train['price']/data_train['sqft_living']
unitPriceAvg = data_train.groupby(['zipcode'],as_index=False)['unitPrice'].mean().sort_values(by='unitPrice', ascending = False)
unitPriceAvg.reset_index(inplace=True)
unitPriceAvg

plt.hist(unitPriceAvg['unitPrice'])
plt.title('Avg Price Distributions')

zip1 = unitPriceAvg[unitPriceAvg['unitPrice']<200]['zipcode']
print(zip1.count())
zip2 = unitPriceAvg[(unitPriceAvg['unitPrice']<300) & (unitPriceAvg['unitPrice']>=200)]['zipcode']
print(zip2.count())
zip3 = unitPriceAvg[(unitPriceAvg['unitPrice']<400) & (unitPriceAvg['unitPrice']>=300)]['zipcode']
print(zip3.count())
zip4 = unitPriceAvg[(unitPriceAvg['unitPrice']<500) & (unitPriceAvg['unitPrice']>=400)]['zipcode']
print(zip4.count())
zip5 = unitPriceAvg[unitPriceAvg['unitPrice']>=500]['zipcode']
print(zip5.count())

print(data_train[data_train['zipcode'].isin(zip1)]['id'].count())
print(data_train[data_train['zipcode'].isin(zip2)]['id'].count())
print(data_train[data_train['zipcode'].isin(zip3)]['id'].count())
print(data_train[data_train['zipcode'].isin(zip4)]['id'].count())
print(data_train[data_train['zipcode'].isin(zip5)]['id'].count())

data_train.loc[data_train['zipcode'].isin(zip1), 'zipGroup']=1
data_train.loc[data_train['zipcode'].isin(zip2), 'zipGroup']=2
data_train.loc[data_train['zipcode'].isin(zip3), 'zipGroup']=3
data_train.loc[data_train['zipcode'].isin(zip4), 'zipGroup']=4
data_train.loc[data_train['zipcode'].isin(zip5), 'zipGroup']=5
data_train.head()

qtyFeatures= ['dateInt','bedrooms','bathrooms','logSqft_Living','logSqft_Lot','floors','waterfront','logSqft_Above','sqft_basement','yr_built',	'yr_renovated','logSqft_Living15','logSqft_Lot15']
catFeatures = ['view', 'condition', 'grade','zipGroup']
X_encoded = encoder(data_train, catFeatures, qtyFeatures)
y_encoded = data_train['logPrice']
X_train, X_test, y_train, y_test = train_test_split(X_encoded,y_encoded,test_size = 0.2,random_state = 1)

polynomial_features= PolynomialFeatures(degree=2)
X_poly = polynomial_features.fit_transform(X_train)
X_poly_test = polynomial_features.fit_transform(X_test)
linreg = LinearRegression()
linreg.fit(X_poly, y_train)
y_predicted_poly = linreg.predict(X_poly_test)
get_rmse(y_predicted_poly, y_test)

"""<h2>
Use statsmodels module
</h2>
"""

import statsmodels.api as sm
from scipy import stats
qtyFeatures= ['dateInt','bedrooms','bathrooms','logSqft_Living','logSqft_Lot','floors','waterfront','logSqft_Above','logSqft_Basement','yr_built',	'yr_renovated','logSqft_Living15','logSqft_Lot15']
catFeatures = ['view', 'condition', 'grade','zipcode']
X_encoded = encoder(data_train, catFeatures, qtyFeatures)
y_encoded = data_train['logPrice']
X101 = sm.add_constant(X_encoded)
est = sm.OLS(y_encoded, X101)
est2 = est.fit()
print(est2.summary())

filteredfeatures=[  'dateInt', 'bedrooms', 'bathrooms', 'floors', 'logSqft_Basement', 'yr_built', 'yr_renovated', 'logSqft_Lot15', 'grade_3', 'grade_4', 'grade_5', 'grade_6', 'grade_7', 'grade_8', 'grade_9', 'grade_10', 'grade_11', 'zipcode_98002', 'zipcode_98003', 'zipcode_98022', 'zipcode_98023', 'zipcode_98030', 'zipcode_98032', 'zipcode_98042', 'zipcode_98092', ]

X_encoded_dropbyP_features=X_encoded[filteredfeatures]

"""Drop by P-value won't improve the accuary"""

Model_Compare_Result_filtered =model_compare(X_encoded_dropbyP_features,y_encoded)
Model_Compare_Result_filtered

"""<h1>
Feature Selection
</h1>

Treat zipcode as one feature
"""

data_train['zipScore']=data_train['zipcode'].apply(lambda x : (int)(unitPriceAvg.loc[unitPriceAvg['zipcode']==x].index.values))

qtyFeatures= ['dateInt','bedrooms','bathrooms','logSqft_Living','logSqft_Lot','floors','waterfront',	'logSqft_Above','logSqft_Basement','yr_built',	'yr_renovated','logSqft_Living15','logSqft_Lot15', 'view', 'condition', 'grade', 'zipScore']
catFeatures=[]
X_encoded = encoder(data_train, catFeatures, qtyFeatures)
y_encoded = data_train['logPrice']
X_encoded.head()

clf = LinearRegression()
backward_feature_selector = SequentialFeatureSelector(clf,
           k_features="best",
           forward=False,
           verbose=2,
           scoring='neg_mean_squared_error',
           cv=5)
backward_features = backward_feature_selector.fit(np.array(X_encoded),y_encoded)
backward_filtered_features= X_encoded.columns[list(backward_features.k_feature_idx_)]
backward_filtered_features

"""Means no feature should be deleted

Treat zipcode as 70 features
"""

X_encoded = preprocessingX(data_train)
y_encoded = preprocessingY(data_train)

clf = LinearRegression()
backward_feature_selector = SequentialFeatureSelector(clf,
           k_features="best",
           forward=False,
           verbose=2,
           scoring='neg_mean_squared_error',
           cv=5)
backward_features = backward_feature_selector.fit(np.array(X_encoded),y_encoded)
backward_filtered_features= X_encoded.columns[list(backward_features.k_feature_idx_)]
backward_filtered_features

backward_filtered_features_array=['dateInt', 'bedrooms', 'bathrooms', 'logSqft_Living', 'logSqft_Lot',
       'floors', 'waterfront', 'logSqft_Above', 'logSqft_Basement', 'yr_built',
       'yr_renovated', 'logSqft_Living15', 'logSqft_Lot15', 'view_1', 'view_2',
       'view_3', 'view_4', 'condition_2', 'condition_3', 'condition_4',
       'condition_5', 'grade_5', 'grade_6', 'grade_8', 'grade_9', 'grade_10',
       'grade_11', 'grade_12', 'grade_13', 'zipcode_98003', 'zipcode_98004',
       'zipcode_98005', 'zipcode_98006', 'zipcode_98007', 'zipcode_98008',
       'zipcode_98010', 'zipcode_98011', 'zipcode_98014', 'zipcode_98019',
       'zipcode_98022', 'zipcode_98023', 'zipcode_98024', 'zipcode_98027',
       'zipcode_98028', 'zipcode_98029', 'zipcode_98030', 'zipcode_98031',
       'zipcode_98033', 'zipcode_98034', 'zipcode_98038', 'zipcode_98039',
       'zipcode_98040', 'zipcode_98042', 'zipcode_98045', 'zipcode_98052',
       'zipcode_98053', 'zipcode_98055', 'zipcode_98056', 'zipcode_98058',
       'zipcode_98059', 'zipcode_98065', 'zipcode_98070', 'zipcode_98072',
       'zipcode_98074', 'zipcode_98075', 'zipcode_98077', 'zipcode_98092',
       'zipcode_98102', 'zipcode_98103', 'zipcode_98105', 'zipcode_98106',
       'zipcode_98107', 'zipcode_98108', 'zipcode_98109', 'zipcode_98112',
       'zipcode_98115', 'zipcode_98116', 'zipcode_98117', 'zipcode_98118',
       'zipcode_98119', 'zipcode_98122', 'zipcode_98125', 'zipcode_98126',
       'zipcode_98133', 'zipcode_98136', 'zipcode_98144', 'zipcode_98146',
       'zipcode_98148', 'zipcode_98155', 'zipcode_98166', 'zipcode_98168',
       'zipcode_98177', 'zipcode_98178', 'zipcode_98188', 'zipcode_98198',
       'zipcode_98199']

len(backward_filtered_features_array)

deleted_feature = list(set(X_encoded.columns) - set(backward_filtered_features_array))
deleted_feature

"""Means that ['grade_7', 'grade_3', 'grade_4', 'zipcode_98002', 'zipcode_98032'] should be deleted"""

X_encoded_filtered=X_encoded[backward_filtered_features_array]
Model_Compare_Result_filtered =model_compare(X_encoded_filtered,y_encoded)
Model_Compare_Result_filtered

print(backward_filtered_features)
print(X_encoded.columns)

clf = LinearRegression()
forward_feature_selector = SequentialFeatureSelector(clf,
           k_features="best",
           forward=True,
           verbose=2,
           scoring='neg_mean_squared_error',
           cv=10)
forward_features = forward_feature_selector.fit(np.array(X_encoded),y_encoded)
forward_filtered_features= X_encoded.columns[list(forward_features.k_feature_idx_)]
forward_filtered_features

print(forward_filtered_features)
print(X_encoded.columns)

"""<h1>
Predict test data set
</h1>
"""

X_target= preprocessingX(data_test)

different = list(set(X_encoded.columns) - set(X_target.columns))
different

"""Error cause by lack of dummy variable grade_3, AND grade_4"""

X_encoded.columns.get_loc("grade_3")

X_encoded.columns.get_loc("grade_4")

testSetLen=len(data_test.index)
Grade_3 = [0]*testSetLen
Grade_4 = [0]*testSetLen

X_target= preprocessingX(data_test)
X_target.insert(21,'grade_3',Grade_3)
X_target.insert(22,'grade_4',Grade_4)

X_target.columns.get_loc("grade_3")

X_target.columns.get_loc("grade_4")

linreg = LinearRegression()
linreg.fit(X_encoded, y_encoded)
y_predicted_log = linreg.predict(X_target)
y_predicted = np.e**(y_predicted_log)

print(y_predicted)

data_test.shape

y_predicted.shape

from google.colab import files

data_test["price"]=y_predicted
data_test.to_csv("result.csv")
files.download('result.csv')
